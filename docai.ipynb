{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13576321,"sourceType":"datasetVersion","datasetId":8624750},{"sourceId":13609057,"sourceType":"datasetVersion","datasetId":8648194},{"sourceId":426330,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":347541,"modelId":368803}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q langchain langchain-community sentence-transformers chromadb transformers accelerate bitsandbytes pypdf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-07T09:58:04.652089Z","iopub.execute_input":"2025-11-07T09:58:04.652319Z","iopub.status.idle":"2025-11-07T09:59:46.182379Z","shell.execute_reply.started":"2025-11-07T09:58:04.652295Z","shell.execute_reply":"2025-11-07T09:59:46.181424Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.6/456.6 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.1/456.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.33.0 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.0 which is incompatible.\ngoogle-cloud-bigtable 2.32.0 requires google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q pymupdf tqdm pandas matplotlib seaborn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T09:59:47.464565Z","iopub.execute_input":"2025-11-07T09:59:47.465379Z","iopub.status.idle":"2025-11-07T09:59:52.756986Z","shell.execute_reply.started":"2025-11-07T09:59:47.465344Z","shell.execute_reply":"2025-11-07T09:59:52.756243Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q sentence-transformers transformers accelerate torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T09:59:52.758279Z","iopub.execute_input":"2025-11-07T09:59:52.758543Z","iopub.status.idle":"2025-11-07T09:59:56.096367Z","shell.execute_reply.started":"2025-11-07T09:59:52.758522Z","shell.execute_reply":"2025-11-07T09:59:56.095370Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport fitz\nimport torch\nimport chromadb\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom chromadb.config import Settings\nfrom langchain.schema import Document\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.schema import HumanMessage, AIMessage\nfrom sentence_transformers import SentenceTransformer\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:00:00.029359Z","iopub.execute_input":"2025-11-07T10:00:00.030028Z","iopub.status.idle":"2025-11-07T10:00:31.251138Z","shell.execute_reply.started":"2025-11-07T10:00:00.029989Z","shell.execute_reply":"2025-11-07T10:00:31.250339Z"}},"outputs":[{"name":"stderr","text":"2025-11-07 10:00:19.031112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762509619.186239      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762509619.236109      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:00:31.252342Z","iopub.execute_input":"2025-11-07T10:00:31.253149Z","iopub.status.idle":"2025-11-07T10:00:31.257523Z","shell.execute_reply.started":"2025-11-07T10:00:31.253130Z","shell.execute_reply":"2025-11-07T10:00:31.256697Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def text_formatter(text: str) -> str:\n    \"\"\"Performs minor formatting on text.\"\"\"\n    cleaned_text = text.replace(\"\\n\", \" \").strip()\n    return cleaned_text\n\ndef open_and_read_pdf(pdf_path: str, source_name: str) -> list[dict]:\n    \"\"\"\n    Opens a PDF file, reads its text content page by page, with source tracking.\n    \n    Returns:\n        list[dict]: Pages with text and metadata (source document name)\n    \"\"\"\n    doc = fitz.open(pdf_path)\n    pages_and_texts = []\n    \n    for page_number, page in tqdm(enumerate(doc), desc=f\"Reading {source_name}\"):\n        text = page.get_text()  # Extract plain text (no images)\n        text = text_formatter(text)\n        \n        pages_and_texts.append({\n            \"page_number\": page_number,\n            \"source\": source_name,  \n            \"page_char_count\": len(text),\n            \"page_word_count\": len(text.split(\" \")),\n            \"page_sentence_count_raw\": len(text.split(\". \")),\n            \"page_token_count\": len(text) / 4, \n            \"text\": text\n        })\n    \n    return pages_and_texts\n\n\npdf_dir = '/kaggle/input/docaidataset'  \n\nloader = DirectoryLoader(\n    pdf_dir,\n    glob=\"*.pdf\",  \n    loader_cls=PyPDFLoader, \n    show_progress=True,\n    use_multithreading=True, \n    max_concurrency=4 \n)\n\ndocuments = loader.load()\n\nprint(f\"âœ… Loaded {len(documents)} documents\")\nprint(f\"Sample doc metadata: {documents[0].metadata if documents else 'No docs'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:00:32.913939Z","iopub.execute_input":"2025-11-07T10:00:32.914180Z","iopub.status.idle":"2025-11-07T10:00:36.486713Z","shell.execute_reply.started":"2025-11-07T10:00:32.914163Z","shell.execute_reply":"2025-11-07T10:00:36.485850Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.40it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… Loaded 98 documents\nSample doc metadata: {'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-11-04T09:35:37+00:00', 'source': '/kaggle/input/docaidataset/react-21-33.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"all_pages_and_texts = []\n\nfor doc in documents:\n    source_name = doc.metadata.get('source', 'Unknown').split('/')[-1]  \n    page_number = doc.metadata.get('page', 0)\n    text = text_formatter(doc.page_content) \n    \n    all_pages_and_texts.append({\n        \"page_number\": page_number,\n        \"source\": source_name,\n        \"page_char_count\": len(text),\n        \"page_word_count\": len(text.split(\" \")),\n        \"page_sentence_count_raw\": len(text.split(\". \")),\n        \"page_token_count\": len(text) / 4,\n        \"text\": text\n    })\n\ndf = pd.DataFrame(all_pages_and_texts)\nprint(f\"\\nDataset Statistics by Source:\")\nprint(df.groupby('source')[['page_char_count', 'page_word_count']].agg(['sum', 'mean']))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:00:38.213885Z","iopub.execute_input":"2025-11-07T10:00:38.214977Z","iopub.status.idle":"2025-11-07T10:00:38.250861Z","shell.execute_reply.started":"2025-11-07T10:00:38.214950Z","shell.execute_reply":"2025-11-07T10:00:38.250221Z"}},"outputs":[{"name":"stdout","text":"\nDataset Statistics by Source:\n                page_char_count              page_word_count            \n                            sum         mean             sum        mean\nsource                                                                  \ncot-1-22.pdf              77449  3520.409091           12437  565.318182\ncot-23-43.pdf             58229  2772.809524           10428  496.571429\ndeepseekocr.pdf           53278  2421.727273            8427  383.045455\nreact-1-20.pdf            77085  3854.250000           10783  539.150000\nreact-21-33.pdf           32976  2536.615385            6000  461.538462\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=512, \n    chunk_overlap=100, \n    length_function=len,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n\ntext_chunks = text_splitter.split_documents(documents)\nchunk_lengths = [len(chunk.page_content) for chunk in text_chunks]\n\nprint(f\"Created {len(text_chunks)} chunks from {len(documents)} documents\")\nprint(f\"Chunk statistics:\")\nprint(f\" - Min: {min(chunk_lengths)} chars\")\nprint(f\" - Max: {max(chunk_lengths)} chars\")\nprint(f\" - Avg: {sum(chunk_lengths)/len(chunk_lengths):.0f} chars\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:00:41.334361Z","iopub.execute_input":"2025-11-07T10:00:41.334653Z","iopub.status.idle":"2025-11-07T10:00:41.356948Z","shell.execute_reply.started":"2025-11-07T10:00:41.334630Z","shell.execute_reply":"2025-11-07T10:00:41.356248Z"}},"outputs":[{"name":"stdout","text":"Created 762 chunks from 98 documents\nChunk statistics:\n - Min: 7 chars\n - Max: 512 chars\n - Avg: 454 chars\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model_name = \"Qwen/Qwen3-Embedding-0.6B\"\n\nembedding_model = SentenceTransformer(\n    model_name,\n    device=device,\n    trust_remote_code=True\n)\n\nprint(f\"\\n Embedding model loaded: {model_name}\")\nprint(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:00:43.754078Z","iopub.execute_input":"2025-11-07T10:00:43.754834Z","iopub.status.idle":"2025-11-07T10:01:01.834756Z","shell.execute_reply.started":"2025-11-07T10:00:43.754804Z","shell.execute_reply":"2025-11-07T10:01:01.833945Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8644427fb25c4ef4bda7f9f35b44087a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b75f572511f49e8bc571de9e9880acd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d534db0d3622408786c35a0707279a3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1cef9c991ec46ec8b305599f2417999"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e524c7aec3f94cc6b004209261f82b20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fb6e1e71317436aa33211a35a20b550"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4270c68d0e649c8b16dac270bcd22fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"329f5c39dee949549b621035a0c9ed32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"241027a35fbb4957ba3baef875db61d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3743c15c33dd4933bfe6fb5458e140cc"}},"metadata":{}},{"name":"stdout","text":"\n Embedding model loaded: Qwen/Qwen3-Embedding-0.6B\nEmbedding dimension: 1024\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"chunk_texts = [chunk.page_content for chunk in text_chunks]\n\nprint(f\"\\nGenerating embeddings for {len(chunk_texts)} chunks...\")\n\nembeddings = embedding_model.encode(\n    chunk_texts,\n    show_progress_bar=True,\n    batch_size=16,\n    normalize_embeddings=True, \n    convert_to_numpy=True\n)\n\nprint(f\" Embeddings generated! Shape: {embeddings.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:01:01.836049Z","iopub.execute_input":"2025-11-07T10:01:01.836386Z","iopub.status.idle":"2025-11-07T10:02:11.924744Z","shell.execute_reply.started":"2025-11-07T10:01:01.836367Z","shell.execute_reply":"2025-11-07T10:02:11.923877Z"}},"outputs":[{"name":"stdout","text":"\nGenerating embeddings for 762 chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966298dcece3453da5c7821f4850e96f"}},"metadata":{}},{"name":"stdout","text":" Embeddings generated! Shape: (762, 1024)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"client = chromadb.PersistentClient(path=\"/kaggle/working/chroma_db\")\ntry:\n    client.delete_collection(name=\"multi_doc_rag\")\nexcept:\n    pass\n\ncollection = client.create_collection(\n    name=\"multi_doc_rag\",\n    metadata={\"description\": \"Multi-document RAG with source tracking\"}\n)\n\ncollection.add(\n    embeddings=embeddings.tolist(),\n    documents=chunk_texts,\n    metadatas=[\n        {\n            \"page_number\": chunk.metadata.get(\"page_number\", 0),  \n            \"source\": chunk.metadata.get(\"source\", \"Unknown\"),     \n            \"page\": chunk.metadata.get(\"page\", 0)\n        } \n        for chunk in text_chunks\n    ],\n    ids=[f\"chunk_{i}\" for i in range(len(text_chunks))]\n)\n\nprint(f\"Added {collection.count()} chunks to vector store\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:02:11.926084Z","iopub.execute_input":"2025-11-07T10:02:11.926375Z","iopub.status.idle":"2025-11-07T10:02:13.058694Z","shell.execute_reply.started":"2025-11-07T10:02:11.926352Z","shell.execute_reply":"2025-11-07T10:02:13.057912Z"}},"outputs":[{"name":"stdout","text":"Added 762 chunks to vector store\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_8bit=True, \n    llm_int8_threshold=6.0,\n    llm_int8_enable_fp32_cpu_offload=False \n)\n\nmodel_name = \"Qwen/Qwen2.5-7B\"\n\nprint(f\"Loading: {model_name}\")\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    trust_remote_code=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16 \n)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nprint(f\" Model loaded! Size: {model.get_memory_footprint() / 1e9:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:02:13.059608Z","iopub.execute_input":"2025-11-07T10:02:13.060132Z","iopub.status.idle":"2025-11-07T10:05:02.518530Z","shell.execute_reply.started":"2025-11-07T10:02:13.060103Z","shell.execute_reply":"2025-11-07T10:05:02.517732Z"}},"outputs":[{"name":"stdout","text":"Loading: Qwen/Qwen2.5-7B\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df3823a83996462db531beea5aaefcca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce52bb0b0a6443efafd6abed41b164e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"722ae18acdf74eeca0fa3d28dd1ba512"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d34f47a0bb5a471aadcad355d2241726"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efc3e473b02044f3ad4eee9945ab51fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3af00ba97a394f3586a5369a2f634524"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25946b74834b4d3296fa24b2d4604d37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72227078a15548df9699900e04d893e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc0a321337444c2097cee4fa79f2810c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8893775eb3347b9af9dfa1434c007b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f287673fd50e4193aa1f03db530d185d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3050fb3cfb847fb99e2d7ddf2918c96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9126ead2a98e4ca69ed01a24e92d0ee7"}},"metadata":{}},{"name":"stdout","text":" Model loaded! Size: 8.71 GB\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"conversation_memory = ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    return_messages=True,\n    output_key=\"answer\",\n    input_key=\"question\",\n    max_token_limit=2000  # Adjust based on your needs\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:26:00.835419Z","iopub.execute_input":"2025-11-07T10:26:00.835947Z","iopub.status.idle":"2025-11-07T10:26:00.840069Z","shell.execute_reply.started":"2025-11-07T10:26:00.835923Z","shell.execute_reply":"2025-11-07T10:26:00.839340Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def rewrite_question(question, memory):\n    \"\"\"\n    Convert follow-up questions into standalone questions using LLM.\n    \"\"\"\n    chat_history = memory.load_memory_variables({}).get(\"chat_history\", [])\n    \n    if not chat_history or len(chat_history) == 0:\n        return question\n    \n    history_text = \"\"\n    for msg in chat_history[-4:]:  \n        if isinstance(msg, HumanMessage):\n            history_text += f\"Q: {msg.content[:100]}\\n\"\n        elif isinstance(msg, AIMessage):\n            history_text += f\"A: {msg.content[:100]}\\n\"\n    \n    rewrite_messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"Rewrite the follow-up question to be standalone. Output ONLY the rewritten question - no explanations.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Context:\n{history_text}\nCurrent question: {question}\n\nRewritten standalone question:\"\"\"\n        }\n    ]\n    \n    prompt = tokenizer.apply_chat_template(\n        rewrite_messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n    input_length = inputs[\"input_ids\"].shape[1]\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=30,  \n            temperature=0.1,    # Deterministic\n            do_sample=False,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=[tokenizer.eos_token_id, 151645],\n        )\n    \n    response_tokens = outputs[0][input_length:]\n    rewritten = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n    \n\n    rewritten = rewritten.split('\\n')[0].strip()\n    \n\n    if (len(rewritten) > 250 or \n        len(rewritten) < 5 or\n        \"assistant\" in rewritten.lower() or\n        \"context:\" in rewritten.lower() or\n        rewritten.count(\"?\") > 2):\n        print(f\" Rewrite validation failed, using original\")\n        return question\n    \n    print(f\" Rewritten: {rewritten}\")\n    return rewritten\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:26:07.219950Z","iopub.execute_input":"2025-11-07T10:26:07.220193Z","iopub.status.idle":"2025-11-07T10:26:07.228191Z","shell.execute_reply.started":"2025-11-07T10:26:07.220178Z","shell.execute_reply":"2025-11-07T10:26:07.227470Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def query_rag_chatbot(question, memory=None, n_results=3, max_new_tokens=512, temperature=0.2, use_rewrite=True, similarity_threshold=0.15):\n    \"\"\"\n    Query RAG with multi-turn conversation and hallucination prevention.\n    \n    Args:\n        question: Current user question\n        memory: ConversationBufferMemory object\n        n_results: Number of retrieved chunks\n        max_new_tokens: Max tokens to generate\n        temperature: Sampling temperature\n        use_rewrite: Whether to rewrite follow-up questions\n        similarity_threshold: Minimum similarity score (default 0.15)\n    \n    Returns:\n        answer: Generated answer\n        sources: Retrieved source information\n    \"\"\"\n    \n\n    retrieval_question = question\n    if use_rewrite and memory:\n        chat_history = memory.load_memory_variables({}).get(\"chat_history\", [])\n        if len(chat_history) > 0:\n            retrieval_question = rewrite_question(question, memory)\n    \n\n    query_embedding = embedding_model.encode(\n        [retrieval_question], \n        normalize_embeddings=True,\n        convert_to_numpy=True\n    )\n    \n\n    results = collection.query(\n        query_embeddings=query_embedding.tolist(),\n        n_results=n_results,\n        include=['documents', 'metadatas', 'distances']\n    )\n    \n\n    retrieved_docs = results['documents'][0]\n    retrieved_metadata = results['metadatas'][0]\n    distances = results['distances'][0]\n    \n\n    similarities = [1 - dist for dist in distances]\n    \n\n    relevant_results = [\n        (doc, meta, sim) \n        for doc, meta, sim in zip(retrieved_docs, retrieved_metadata, similarities)\n        if sim >= similarity_threshold\n    ]\n    \n\n    if not relevant_results:\n        no_context_answer = \"I cannot find relevant information in the provided documents to answer this question. Please ask something related to the document content.\"\n        \n        if memory is not None:\n            memory.save_context(\n                {\"question\": question},\n                {\"answer\": no_context_answer}\n            )\n        \n        return no_context_answer, []\n    \n\n    filtered_docs = [item[0] for item in relevant_results]\n    filtered_metadata = [item[1] for item in relevant_results]\n    filtered_similarities = [item[2] for item in relevant_results]\n    \n    context = \"\\n\\n\".join([\n        f\"[Source: {meta['source']}, Page {meta.get('page', 0) + 1}]\\n{doc}\"\n        for doc, meta in zip(filtered_docs, filtered_metadata)\n    ])\n    \n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"You are a helpful AI assistant specialized in answering questions based strictly on provided context.\n\nSTRICT RULES:\n1. Answer ONLY using information explicitly stated in the Context section\n2. If the context doesn't contain the answer, respond with: \"I cannot find this information in the provided documents.\"\n3. DO NOT use your general knowledge or training data\n4. DO NOT make up information, names, dates, or facts\n5. When referencing information, cite the source document\n\nIf the question is outside the scope of the provided documents, politely decline to answer.\"\"\"\n        }\n    ]\n    \n\n    if memory is not None:\n        chat_history = memory.load_memory_variables({}).get(\"chat_history\", [])\n        \n        for msg in chat_history[-6:]: \n            if isinstance(msg, HumanMessage):\n                messages.append({\"role\": \"user\", \"content\": msg.content})\n            elif isinstance(msg, AIMessage):\n                messages.append({\"role\": \"assistant\", \"content\": msg.content})\n    \n\n    messages.append({\n        \"role\": \"user\",\n        \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n    })\n\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(model.device)\n    input_length = inputs[\"input_ids\"].shape[1]\n    \n    stop_token_ids = [tokenizer.eos_token_id, 151645]\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.1,\n            top_p=0.85,    \n            do_sample=True,\n            repetition_penalty=1.1, \n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=stop_token_ids\n        )\n    \n    response_tokens = outputs[0][input_length:]\n    answer = tokenizer.decode(response_tokens, skip_special_tokens=True)\n\n    if memory is not None:\n        memory.save_context(\n            {\"question\": question},\n            {\"answer\": answer}\n        )\n    \n\n    sources = [\n        {\n            'source': meta.get('source', 'Unknown'),\n            'page': meta.get('page', 0) + 1,\n            'similarity': sim,\n            'text_preview': doc[:100]\n        }\n        for doc, meta, sim in zip(filtered_docs, filtered_metadata, filtered_similarities)\n    ]\n    \n    return answer, sources\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:26:11.676238Z","iopub.execute_input":"2025-11-07T10:26:11.676883Z","iopub.status.idle":"2025-11-07T10:26:11.689231Z","shell.execute_reply.started":"2025-11-07T10:26:11.676858Z","shell.execute_reply":"2025-11-07T10:26:11.688595Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Reinitialize memory for clean start\nconversation_memory = ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    return_messages=True,\n    output_key=\"answer\",\n    input_key=\"question\"\n)\n\nprint(\"ğŸ¤– Advanced Multi-turn RAG Chatbot with Hallucination Prevention\")\nprint(\"Type 'quit' to exit, 'clear' to reset history\\n\" + \"=\"*70)\n\nwhile True:\n    question = input(\"\\nğŸ§‘ You: \").strip()\n    \n    if question.lower() in ['quit', 'exit', 'q']:\n        print(\"ğŸ‘‹ Goodbye!\")\n        break\n    \n    if question.lower() == 'clear':\n        conversation_memory.clear()\n        print(\"ğŸ—‘ï¸ [Conversation history cleared]\")\n        continue\n    \n    if not question:\n        continue\n    \n    print(\"\\nâ³ [Processing...]\")\n    \n    answer, sources = query_rag_chatbot(\n        question, \n        memory=conversation_memory,\n        n_results=6,\n        use_rewrite=True,\n        similarity_threshold=0.15  # ADDED threshold parameter\n    )\n    \n    print(f\"\\nğŸ¤– Assistant: {answer}\")\n    \n\n    if sources:\n        print(\"\\nğŸ“š Sources:\")\n        for i, source in enumerate(sources, 1):\n            quality = \"ğŸŸ¢\" if source['similarity'] > 0.3 else \"ğŸŸ¡\" if source['similarity'] > 0.15 else \"ğŸ”´\"\n            print(f\"{i}. {quality} {source['source']} (Page {source['page']}) - Relevance: {source['similarity']:.3f}\")\n            print(f\"   Preview: {source['text_preview']}...\")\n    else:\n        print(\"\\n No relevant sources found - answer is based on rejection of irrelevant context\")\n    \n    history_len = len(conversation_memory.load_memory_variables({}).get(\"chat_history\", []))\n    print(f\"\\nğŸ’¬ [Chat history: {history_len//2} turn(s)]\")\n    print(\"=\"*70)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:26:16.754042Z","iopub.execute_input":"2025-11-07T10:26:16.754331Z","iopub.status.idle":"2025-11-07T10:29:52.817135Z","shell.execute_reply.started":"2025-11-07T10:26:16.754311Z","shell.execute_reply":"2025-11-07T10:29:52.816456Z"}},"outputs":[{"name":"stdout","text":"ğŸ¤– Advanced Multi-turn RAG Chatbot with Hallucination Prevention\nType 'quit' to exit, 'clear' to reset history\n======================================================================\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nğŸ§‘ You:  what is chain of thought?\n"},{"name":"stdout","text":"\nâ³ [Processing...]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc31fb8e11e4eaab2e9cf24507c6ac9"}},"metadata":{}},{"name":"stdout","text":"\nğŸ¤– Assistant: Chain-of-thought prompting involves providing a step-by-step thought process for arriving at the answer, mimicking how humans reason through problems. This method helps large language models perform complex arithmetic, commonsense, and symbolic reasoning tasks by breaking down multi-step problems into smaller parts. It's particularly useful because it encourages models to think sequentially and justify their answers, making them more reliable and interpretable.\n\nğŸ“š Sources:\n1. ğŸŸ¢ /kaggle/input/docaidataset/cot-1-22.pdf (Page 1) - Relevance: 0.345\n   Preview: Figure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\ncom...\n2. ğŸŸ¡ /kaggle/input/docaidataset/cot-1-22.pdf (Page 3) - Relevance: 0.226\n   Preview: mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\nt...\n3. ğŸŸ¡ /kaggle/input/docaidataset/cot-1-22.pdf (Page 3) - Relevance: 0.215\n   Preview: language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\np...\n4. ğŸŸ¡ /kaggle/input/docaidataset/cot-1-22.pdf (Page 3) - Relevance: 0.166\n   Preview: with chains of thought for promptingâ€”Figure 1 (right) shows one chain of thought exemplar, and the\nf...\n5. ğŸŸ¡ /kaggle/input/docaidataset/cot-23-43.pdf (Page 1) - Relevance: 0.159\n   Preview: Table 6: Ablation and robustness results for arithmetic reasoning datasets. Chain of thought general...\n6. ğŸŸ¡ /kaggle/input/docaidataset/cot-1-22.pdf (Page 6) - Relevance: 0.155\n   Preview: works for other sets of exemplars, we also run experiments\nwith three sets of eight exemplars random...\n\nğŸ’¬ [Chat history: 1 turn(s)]\n======================================================================\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nğŸ§‘ You:  what are it's benefits?\n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"\nâ³ [Processing...]\n Rewritten: Rewrite the follow-up question to be standalone. Output ONLY the rewritten question - no explanations. Output ONLY the rewritten question - no explanations.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a6909a78b6b430d825d00e00cc27786"}},"metadata":{}},{"name":"stdout","text":"\nğŸ¤– Assistant: I cannot find relevant information in the provided documents to answer this question. Please ask something related to the document content.\n\n No relevant sources found - answer is based on rejection of irrelevant context\n\nğŸ’¬ [Chat history: 2 turn(s)]\n======================================================================\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nğŸ§‘ You:  how is ReAct better than CoT\n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"\nâ³ [Processing...]\n Rewrite validation failed, using original\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9afd57716941ed85308ecc21e0c7d3"}},"metadata":{}},{"name":"stdout","text":"\nğŸ¤– Assistant: ReAct is better than CoT in terms of being more factual and grounded while still maintaining accuracy in formulating reasoning structures. Additionally, ReAct incorporates elements from both ReAct and CoT-SC, allowing it to adaptively choose the appropriate method depending on whether it needs to rely on ReAct or CoT-SC. This flexibility enables ReAct to handle various types of success and failure modes effectively across different datasets like HotpotQA and Fever.\n\nğŸ“š Sources:\n1. ğŸŸ¢ /kaggle/input/docaidataset/react-1-20.pdf (Page 5) - Relevance: 0.491\n   Preview: the problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\nis more ...\n2. ğŸŸ¢ /kaggle/input/docaidataset/react-1-20.pdf (Page 6) - Relevance: 0.459\n   Preview: Hallucination Hallucinated reasoning trace or facts 0% 56%\nLabel ambiguity Right prediction but did ...\n3. ğŸŸ¢ /kaggle/input/docaidataset/react-1-20.pdf (Page 8) - Relevance: 0.419\n   Preview: comparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.\nOn Webshop, one-shot Act ...\n4. ğŸŸ¢ /kaggle/input/docaidataset/react-1-20.pdf (Page 4) - Relevance: 0.418\n   Preview: Published as a conference paper at ICLR 2023\nappear sparsely in the most relevant positions of a tra...\n5. ğŸŸ¢ /kaggle/input/docaidataset/react-1-20.pdf (Page 5) - Relevance: 0.408\n   Preview: input questions/claims. More details are in Appendix B.1.\n3.3 R ESULTS AND OBSERVATIONS\nReAct outper...\n6. ğŸŸ¢ /kaggle/input/docaidataset/react-1-20.pdf (Page 6) - Relevance: 0.402\n   Preview: lags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only\ndiffer by ...\n\nğŸ’¬ [Chat history: 3 turn(s)]\n======================================================================\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nğŸ§‘ You:  exit\n"},{"name":"stdout","text":"ğŸ‘‹ Goodbye!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}